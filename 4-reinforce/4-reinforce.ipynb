{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sr-Nuyb1dBm0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7fd5c056ac40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8ZVi-uydpgL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "!pip install git+https://github.com/simoninithomas/gym-games\n",
    "!pip install huggingface_hub\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install pyyaml==6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U gym==0.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V8oadoJSWp7C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kaJu5FeZxXGY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U5TNYa14aRav",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Agent: CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "POOOk15_K6KA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FMLFrjiBNLYJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  4\n",
      "Sample observation [ 4.6159973e+00 -4.2668840e+37 -7.9091087e-02  9.7825327e+37]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lu6t4sRNNWkN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  2\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ho_UHf49N9i4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NCNvyElRStWG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )\n",
    "\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "utRe1NgtVBYF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 32,\n",
    "    \"n_training_episodes\": 50_000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D3lWyVXBVfl6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uGf-hQCnfouB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 155.14\n",
      "Episode 200\tAverage Score: 110.21\n",
      "Episode 300\tAverage Score: 377.55\n",
      "Episode 400\tAverage Score: 465.76\n",
      "Episode 500\tAverage Score: 445.65\n",
      "Episode 600\tAverage Score: 101.47\n",
      "Episode 700\tAverage Score: 115.25\n",
      "Episode 800\tAverage Score: 102.12\n",
      "Episode 900\tAverage Score: 124.23\n",
      "Episode 1000\tAverage Score: 188.68\n",
      "Episode 1100\tAverage Score: 488.21\n",
      "Episode 1200\tAverage Score: 472.66\n",
      "Episode 1300\tAverage Score: 500.00\n",
      "Episode 1400\tAverage Score: 331.85\n",
      "Episode 1500\tAverage Score: 500.00\n",
      "Episode 1600\tAverage Score: 500.00\n",
      "Episode 1700\tAverage Score: 500.00\n",
      "Episode 1800\tAverage Score: 500.00\n",
      "Episode 1900\tAverage Score: 494.05\n",
      "Episode 2000\tAverage Score: 500.00\n",
      "Episode 2100\tAverage Score: 500.00\n",
      "Episode 2200\tAverage Score: 497.14\n",
      "Episode 2300\tAverage Score: 384.01\n",
      "Episode 2400\tAverage Score: 491.81\n",
      "Episode 2500\tAverage Score: 498.53\n",
      "Episode 2600\tAverage Score: 187.32\n",
      "Episode 2700\tAverage Score: 112.71\n",
      "Episode 2800\tAverage Score: 207.98\n",
      "Episode 2900\tAverage Score: 500.00\n",
      "Episode 3000\tAverage Score: 460.16\n",
      "Episode 3100\tAverage Score: 333.04\n",
      "Episode 3200\tAverage Score: 105.15\n",
      "Episode 3300\tAverage Score: 131.23\n",
      "Episode 3400\tAverage Score: 171.03\n",
      "Episode 3500\tAverage Score: 369.99\n",
      "Episode 3600\tAverage Score: 445.78\n",
      "Episode 3700\tAverage Score: 462.76\n",
      "Episode 3800\tAverage Score: 500.00\n",
      "Episode 3900\tAverage Score: 500.00\n",
      "Episode 4000\tAverage Score: 500.00\n",
      "Episode 4100\tAverage Score: 500.00\n",
      "Episode 4200\tAverage Score: 500.00\n",
      "Episode 4300\tAverage Score: 500.00\n",
      "Episode 4400\tAverage Score: 500.00\n",
      "Episode 4500\tAverage Score: 500.00\n",
      "Episode 4600\tAverage Score: 500.00\n",
      "Episode 4700\tAverage Score: 500.00\n",
      "Episode 4800\tAverage Score: 500.00\n",
      "Episode 4900\tAverage Score: 500.00\n",
      "Episode 5000\tAverage Score: 500.00\n",
      "Episode 5100\tAverage Score: 500.00\n",
      "Episode 5200\tAverage Score: 404.32\n",
      "Episode 5300\tAverage Score: 204.58\n",
      "Episode 5400\tAverage Score: 470.10\n",
      "Episode 5500\tAverage Score: 445.59\n",
      "Episode 5600\tAverage Score: 422.89\n",
      "Episode 5700\tAverage Score: 482.39\n",
      "Episode 5800\tAverage Score: 500.00\n",
      "Episode 5900\tAverage Score: 492.20\n",
      "Episode 6000\tAverage Score: 500.00\n",
      "Episode 6100\tAverage Score: 500.00\n",
      "Episode 6200\tAverage Score: 500.00\n",
      "Episode 6300\tAverage Score: 500.00\n",
      "Episode 6400\tAverage Score: 500.00\n",
      "Episode 6500\tAverage Score: 500.00\n",
      "Episode 6600\tAverage Score: 500.00\n",
      "Episode 6700\tAverage Score: 500.00\n",
      "Episode 6800\tAverage Score: 466.67\n",
      "Episode 6900\tAverage Score: 500.00\n",
      "Episode 7000\tAverage Score: 500.00\n",
      "Episode 7100\tAverage Score: 500.00\n",
      "Episode 7200\tAverage Score: 500.00\n",
      "Episode 7300\tAverage Score: 500.00\n",
      "Episode 7400\tAverage Score: 500.00\n",
      "Episode 7500\tAverage Score: 500.00\n",
      "Episode 7600\tAverage Score: 500.00\n",
      "Episode 7700\tAverage Score: 500.00\n",
      "Episode 7800\tAverage Score: 500.00\n",
      "Episode 7900\tAverage Score: 500.00\n",
      "Episode 8000\tAverage Score: 500.00\n",
      "Episode 8100\tAverage Score: 500.00\n",
      "Episode 8200\tAverage Score: 500.00\n",
      "Episode 8300\tAverage Score: 500.00\n",
      "Episode 8400\tAverage Score: 500.00\n",
      "Episode 8500\tAverage Score: 500.00\n",
      "Episode 8600\tAverage Score: 486.90\n",
      "Episode 8700\tAverage Score: 422.30\n",
      "Episode 8800\tAverage Score: 500.00\n",
      "Episode 8900\tAverage Score: 500.00\n",
      "Episode 9000\tAverage Score: 500.00\n",
      "Episode 9100\tAverage Score: 480.68\n",
      "Episode 9200\tAverage Score: 432.63\n",
      "Episode 9300\tAverage Score: 480.80\n",
      "Episode 9400\tAverage Score: 495.22\n",
      "Episode 9500\tAverage Score: 364.08\n",
      "Episode 9600\tAverage Score: 485.60\n",
      "Episode 9700\tAverage Score: 432.76\n",
      "Episode 9800\tAverage Score: 397.38\n",
      "Episode 9900\tAverage Score: 486.50\n",
      "Episode 10000\tAverage Score: 500.00\n",
      "Episode 10100\tAverage Score: 500.00\n",
      "Episode 10200\tAverage Score: 500.00\n",
      "Episode 10300\tAverage Score: 500.00\n",
      "Episode 10400\tAverage Score: 500.00\n",
      "Episode 10500\tAverage Score: 500.00\n",
      "Episode 10600\tAverage Score: 500.00\n",
      "Episode 10700\tAverage Score: 500.00\n",
      "Episode 10800\tAverage Score: 500.00\n",
      "Episode 10900\tAverage Score: 500.00\n",
      "Episode 11000\tAverage Score: 500.00\n",
      "Episode 11100\tAverage Score: 500.00\n",
      "Episode 11200\tAverage Score: 348.37\n",
      "Episode 11300\tAverage Score: 237.18\n",
      "Episode 11400\tAverage Score: 190.54\n",
      "Episode 11500\tAverage Score: 241.68\n",
      "Episode 11600\tAverage Score: 462.27\n",
      "Episode 11700\tAverage Score: 500.00\n",
      "Episode 11800\tAverage Score: 500.00\n",
      "Episode 11900\tAverage Score: 500.00\n",
      "Episode 12000\tAverage Score: 500.00\n",
      "Episode 12100\tAverage Score: 500.00\n",
      "Episode 12200\tAverage Score: 500.00\n",
      "Episode 12300\tAverage Score: 500.00\n",
      "Episode 12400\tAverage Score: 500.00\n",
      "Episode 12500\tAverage Score: 500.00\n",
      "Episode 12600\tAverage Score: 500.00\n",
      "Episode 12700\tAverage Score: 500.00\n",
      "Episode 12800\tAverage Score: 500.00\n",
      "Episode 12900\tAverage Score: 500.00\n",
      "Episode 13000\tAverage Score: 500.00\n",
      "Episode 13100\tAverage Score: 500.00\n",
      "Episode 13200\tAverage Score: 491.36\n",
      "Episode 13300\tAverage Score: 500.00\n",
      "Episode 13400\tAverage Score: 500.00\n",
      "Episode 13500\tAverage Score: 500.00\n",
      "Episode 13600\tAverage Score: 500.00\n",
      "Episode 13700\tAverage Score: 500.00\n",
      "Episode 13800\tAverage Score: 500.00\n",
      "Episode 13900\tAverage Score: 500.00\n",
      "Episode 14000\tAverage Score: 491.64\n",
      "Episode 14100\tAverage Score: 500.00\n",
      "Episode 14200\tAverage Score: 500.00\n",
      "Episode 14300\tAverage Score: 500.00\n",
      "Episode 14400\tAverage Score: 500.00\n",
      "Episode 14500\tAverage Score: 500.00\n",
      "Episode 14600\tAverage Score: 500.00\n",
      "Episode 14700\tAverage Score: 500.00\n",
      "Episode 14800\tAverage Score: 500.00\n",
      "Episode 14900\tAverage Score: 500.00\n",
      "Episode 15000\tAverage Score: 500.00\n",
      "Episode 15100\tAverage Score: 500.00\n",
      "Episode 15200\tAverage Score: 500.00\n",
      "Episode 15300\tAverage Score: 500.00\n",
      "Episode 15400\tAverage Score: 500.00\n",
      "Episode 15500\tAverage Score: 500.00\n",
      "Episode 15600\tAverage Score: 500.00\n",
      "Episode 15700\tAverage Score: 500.00\n",
      "Episode 15800\tAverage Score: 500.00\n",
      "Episode 15900\tAverage Score: 500.00\n",
      "Episode 16000\tAverage Score: 500.00\n",
      "Episode 16100\tAverage Score: 500.00\n",
      "Episode 16200\tAverage Score: 500.00\n",
      "Episode 16300\tAverage Score: 500.00\n",
      "Episode 16400\tAverage Score: 500.00\n",
      "Episode 16500\tAverage Score: 500.00\n",
      "Episode 16600\tAverage Score: 500.00\n",
      "Episode 16700\tAverage Score: 500.00\n",
      "Episode 16800\tAverage Score: 500.00\n",
      "Episode 16900\tAverage Score: 500.00\n",
      "Episode 17000\tAverage Score: 500.00\n",
      "Episode 17100\tAverage Score: 500.00\n",
      "Episode 17200\tAverage Score: 500.00\n",
      "Episode 17300\tAverage Score: 500.00\n",
      "Episode 17400\tAverage Score: 500.00\n",
      "Episode 17500\tAverage Score: 500.00\n",
      "Episode 17600\tAverage Score: 500.00\n",
      "Episode 17700\tAverage Score: 305.76\n",
      "Episode 17800\tAverage Score: 500.00\n",
      "Episode 17900\tAverage Score: 500.00\n",
      "Episode 18000\tAverage Score: 500.00\n",
      "Episode 18100\tAverage Score: 421.24\n",
      "Episode 18200\tAverage Score: 500.00\n",
      "Episode 18300\tAverage Score: 500.00\n",
      "Episode 18400\tAverage Score: 500.00\n",
      "Episode 18500\tAverage Score: 500.00\n",
      "Episode 18600\tAverage Score: 500.00\n",
      "Episode 18700\tAverage Score: 500.00\n",
      "Episode 18800\tAverage Score: 500.00\n",
      "Episode 18900\tAverage Score: 500.00\n",
      "Episode 19000\tAverage Score: 500.00\n",
      "Episode 19100\tAverage Score: 500.00\n",
      "Episode 19200\tAverage Score: 500.00\n",
      "Episode 19300\tAverage Score: 452.02\n",
      "Episode 19400\tAverage Score: 465.37\n",
      "Episode 19500\tAverage Score: 500.00\n",
      "Episode 19600\tAverage Score: 500.00\n",
      "Episode 19700\tAverage Score: 500.00\n",
      "Episode 19800\tAverage Score: 500.00\n",
      "Episode 19900\tAverage Score: 500.00\n",
      "Episode 20000\tAverage Score: 202.31\n",
      "Episode 20100\tAverage Score: 500.00\n",
      "Episode 20200\tAverage Score: 471.80\n",
      "Episode 20300\tAverage Score: 155.05\n",
      "Episode 20400\tAverage Score: 466.31\n",
      "Episode 20500\tAverage Score: 500.00\n",
      "Episode 20600\tAverage Score: 500.00\n",
      "Episode 20700\tAverage Score: 489.81\n",
      "Episode 20800\tAverage Score: 497.75\n",
      "Episode 20900\tAverage Score: 500.00\n",
      "Episode 21000\tAverage Score: 500.00\n",
      "Episode 21100\tAverage Score: 500.00\n",
      "Episode 21200\tAverage Score: 500.00\n",
      "Episode 21300\tAverage Score: 500.00\n",
      "Episode 21400\tAverage Score: 422.11\n",
      "Episode 21500\tAverage Score: 500.00\n",
      "Episode 21600\tAverage Score: 448.91\n",
      "Episode 21700\tAverage Score: 500.00\n",
      "Episode 21800\tAverage Score: 500.00\n",
      "Episode 21900\tAverage Score: 500.00\n",
      "Episode 22000\tAverage Score: 500.00\n",
      "Episode 22100\tAverage Score: 489.37\n",
      "Episode 22200\tAverage Score: 486.79\n",
      "Episode 22300\tAverage Score: 497.13\n",
      "Episode 22400\tAverage Score: 500.00\n",
      "Episode 22500\tAverage Score: 500.00\n",
      "Episode 22600\tAverage Score: 500.00\n",
      "Episode 22700\tAverage Score: 500.00\n",
      "Episode 22800\tAverage Score: 500.00\n",
      "Episode 22900\tAverage Score: 500.00\n",
      "Episode 23000\tAverage Score: 500.00\n",
      "Episode 23100\tAverage Score: 500.00\n",
      "Episode 23200\tAverage Score: 500.00\n",
      "Episode 23300\tAverage Score: 500.00\n",
      "Episode 23400\tAverage Score: 500.00\n",
      "Episode 23500\tAverage Score: 500.00\n",
      "Episode 23600\tAverage Score: 500.00\n",
      "Episode 23700\tAverage Score: 500.00\n",
      "Episode 23800\tAverage Score: 500.00\n",
      "Episode 23900\tAverage Score: 500.00\n",
      "Episode 24000\tAverage Score: 500.00\n",
      "Episode 24100\tAverage Score: 500.00\n",
      "Episode 24200\tAverage Score: 500.00\n",
      "Episode 24300\tAverage Score: 500.00\n",
      "Episode 24400\tAverage Score: 500.00\n",
      "Episode 24500\tAverage Score: 500.00\n",
      "Episode 24600\tAverage Score: 500.00\n",
      "Episode 24700\tAverage Score: 500.00\n",
      "Episode 24800\tAverage Score: 484.86\n",
      "Episode 24900\tAverage Score: 470.50\n",
      "Episode 25000\tAverage Score: 500.00\n",
      "Episode 25100\tAverage Score: 401.07\n",
      "Episode 25200\tAverage Score: 131.37\n",
      "Episode 25300\tAverage Score: 478.46\n",
      "Episode 25400\tAverage Score: 500.00\n",
      "Episode 25500\tAverage Score: 500.00\n",
      "Episode 25600\tAverage Score: 500.00\n",
      "Episode 25700\tAverage Score: 500.00\n",
      "Episode 25800\tAverage Score: 500.00\n",
      "Episode 25900\tAverage Score: 438.39\n",
      "Episode 26000\tAverage Score: 450.85\n",
      "Episode 26100\tAverage Score: 405.18\n",
      "Episode 26200\tAverage Score: 495.12\n",
      "Episode 26300\tAverage Score: 495.12\n",
      "Episode 26400\tAverage Score: 500.00\n",
      "Episode 26500\tAverage Score: 500.00\n",
      "Episode 26600\tAverage Score: 500.00\n",
      "Episode 26700\tAverage Score: 500.00\n",
      "Episode 26800\tAverage Score: 500.00\n",
      "Episode 26900\tAverage Score: 500.00\n",
      "Episode 27000\tAverage Score: 500.00\n",
      "Episode 27100\tAverage Score: 500.00\n",
      "Episode 27200\tAverage Score: 500.00\n",
      "Episode 27300\tAverage Score: 495.81\n",
      "Episode 27400\tAverage Score: 499.93\n",
      "Episode 27500\tAverage Score: 500.00\n",
      "Episode 27600\tAverage Score: 500.00\n",
      "Episode 27700\tAverage Score: 500.00\n",
      "Episode 27800\tAverage Score: 500.00\n",
      "Episode 27900\tAverage Score: 493.28\n",
      "Episode 28000\tAverage Score: 485.63\n",
      "Episode 28100\tAverage Score: 408.23\n",
      "Episode 28200\tAverage Score: 493.99\n",
      "Episode 28300\tAverage Score: 495.94\n",
      "Episode 28400\tAverage Score: 500.00\n",
      "Episode 28500\tAverage Score: 500.00\n",
      "Episode 28600\tAverage Score: 500.00\n",
      "Episode 28700\tAverage Score: 500.00\n",
      "Episode 28800\tAverage Score: 500.00\n",
      "Episode 28900\tAverage Score: 500.00\n",
      "Episode 29000\tAverage Score: 500.00\n",
      "Episode 29100\tAverage Score: 500.00\n",
      "Episode 29200\tAverage Score: 324.19\n",
      "Episode 29300\tAverage Score: 475.76\n",
      "Episode 29400\tAverage Score: 451.23\n",
      "Episode 29500\tAverage Score: 480.47\n",
      "Episode 29600\tAverage Score: 480.53\n",
      "Episode 29700\tAverage Score: 412.12\n",
      "Episode 29800\tAverage Score: 373.31\n",
      "Episode 29900\tAverage Score: 201.30\n",
      "Episode 30000\tAverage Score: 9.42\n",
      "Episode 30100\tAverage Score: 9.49\n",
      "Episode 30200\tAverage Score: 9.76\n",
      "Episode 30300\tAverage Score: 9.35\n",
      "Episode 30400\tAverage Score: 9.37\n",
      "Episode 30500\tAverage Score: 9.67\n",
      "Episode 30600\tAverage Score: 14.43\n",
      "Episode 30700\tAverage Score: 24.20\n",
      "Episode 30800\tAverage Score: 94.15\n",
      "Episode 30900\tAverage Score: 279.97\n",
      "Episode 31000\tAverage Score: 372.97\n",
      "Episode 31100\tAverage Score: 324.19\n",
      "Episode 31200\tAverage Score: 113.68\n",
      "Episode 31300\tAverage Score: 246.29\n",
      "Episode 31400\tAverage Score: 222.32\n",
      "Episode 31500\tAverage Score: 432.00\n",
      "Episode 31600\tAverage Score: 475.72\n",
      "Episode 31700\tAverage Score: 412.16\n",
      "Episode 31800\tAverage Score: 363.70\n",
      "Episode 31900\tAverage Score: 446.52\n",
      "Episode 32000\tAverage Score: 237.69\n",
      "Episode 32100\tAverage Score: 82.45\n",
      "Episode 32200\tAverage Score: 417.33\n",
      "Episode 32300\tAverage Score: 500.00\n",
      "Episode 32400\tAverage Score: 490.33\n",
      "Episode 32500\tAverage Score: 485.48\n",
      "Episode 32600\tAverage Score: 495.18\n",
      "Episode 32700\tAverage Score: 480.72\n",
      "Episode 32800\tAverage Score: 451.71\n",
      "Episode 32900\tAverage Score: 417.98\n",
      "Episode 33000\tAverage Score: 500.00\n",
      "Episode 33100\tAverage Score: 446.83\n",
      "Episode 33200\tAverage Score: 402.42\n",
      "Episode 33300\tAverage Score: 229.70\n",
      "Episode 33400\tAverage Score: 291.36\n",
      "Episode 33500\tAverage Score: 465.85\n",
      "Episode 33600\tAverage Score: 500.00\n",
      "Episode 33700\tAverage Score: 500.00\n",
      "Episode 33800\tAverage Score: 500.00\n",
      "Episode 33900\tAverage Score: 500.00\n",
      "Episode 34000\tAverage Score: 500.00\n",
      "Episode 34100\tAverage Score: 500.00\n",
      "Episode 34200\tAverage Score: 500.00\n",
      "Episode 34300\tAverage Score: 500.00\n",
      "Episode 34400\tAverage Score: 500.00\n",
      "Episode 34500\tAverage Score: 500.00\n",
      "Episode 34600\tAverage Score: 500.00\n",
      "Episode 34700\tAverage Score: 500.00\n",
      "Episode 34800\tAverage Score: 500.00\n",
      "Episode 34900\tAverage Score: 500.00\n",
      "Episode 35000\tAverage Score: 500.00\n",
      "Episode 35100\tAverage Score: 500.00\n",
      "Episode 35200\tAverage Score: 500.00\n",
      "Episode 35300\tAverage Score: 500.00\n",
      "Episode 35400\tAverage Score: 500.00\n",
      "Episode 35500\tAverage Score: 500.00\n",
      "Episode 35600\tAverage Score: 500.00\n",
      "Episode 35700\tAverage Score: 500.00\n",
      "Episode 35800\tAverage Score: 500.00\n",
      "Episode 35900\tAverage Score: 500.00\n",
      "Episode 36000\tAverage Score: 500.00\n",
      "Episode 36100\tAverage Score: 500.00\n",
      "Episode 36200\tAverage Score: 500.00\n",
      "Episode 36300\tAverage Score: 500.00\n",
      "Episode 36400\tAverage Score: 500.00\n",
      "Episode 36500\tAverage Score: 500.00\n",
      "Episode 36600\tAverage Score: 500.00\n",
      "Episode 36700\tAverage Score: 500.00\n",
      "Episode 36800\tAverage Score: 500.00\n",
      "Episode 36900\tAverage Score: 500.00\n",
      "Episode 37000\tAverage Score: 500.00\n",
      "Episode 37100\tAverage Score: 500.00\n",
      "Episode 37200\tAverage Score: 500.00\n",
      "Episode 37300\tAverage Score: 500.00\n",
      "Episode 37400\tAverage Score: 500.00\n",
      "Episode 37500\tAverage Score: 500.00\n",
      "Episode 37600\tAverage Score: 485.36\n",
      "Episode 37700\tAverage Score: 495.12\n",
      "Episode 37800\tAverage Score: 500.00\n",
      "Episode 37900\tAverage Score: 500.00\n",
      "Episode 38000\tAverage Score: 500.00\n",
      "Episode 38100\tAverage Score: 500.00\n",
      "Episode 38200\tAverage Score: 500.00\n",
      "Episode 38300\tAverage Score: 500.00\n",
      "Episode 38400\tAverage Score: 500.00\n",
      "Episode 38500\tAverage Score: 500.00\n",
      "Episode 38600\tAverage Score: 500.00\n",
      "Episode 38700\tAverage Score: 500.00\n",
      "Episode 38800\tAverage Score: 500.00\n",
      "Episode 38900\tAverage Score: 500.00\n",
      "Episode 39000\tAverage Score: 500.00\n",
      "Episode 39100\tAverage Score: 500.00\n",
      "Episode 39200\tAverage Score: 500.00\n",
      "Episode 39300\tAverage Score: 500.00\n",
      "Episode 39400\tAverage Score: 500.00\n",
      "Episode 39500\tAverage Score: 500.00\n",
      "Episode 39600\tAverage Score: 500.00\n",
      "Episode 39700\tAverage Score: 500.00\n",
      "Episode 39800\tAverage Score: 500.00\n",
      "Episode 39900\tAverage Score: 500.00\n",
      "Episode 40000\tAverage Score: 500.00\n",
      "Episode 40100\tAverage Score: 500.00\n",
      "Episode 40200\tAverage Score: 500.00\n",
      "Episode 40300\tAverage Score: 500.00\n",
      "Episode 40400\tAverage Score: 500.00\n",
      "Episode 40500\tAverage Score: 500.00\n",
      "Episode 40600\tAverage Score: 500.00\n",
      "Episode 40700\tAverage Score: 500.00\n",
      "Episode 40800\tAverage Score: 500.00\n",
      "Episode 40900\tAverage Score: 500.00\n",
      "Episode 41000\tAverage Score: 500.00\n",
      "Episode 41100\tAverage Score: 500.00\n",
      "Episode 41200\tAverage Score: 500.00\n",
      "Episode 41300\tAverage Score: 500.00\n",
      "Episode 41400\tAverage Score: 500.00\n",
      "Episode 41500\tAverage Score: 467.32\n",
      "Episode 41600\tAverage Score: 371.99\n",
      "Episode 41700\tAverage Score: 114.74\n",
      "Episode 41800\tAverage Score: 112.19\n",
      "Episode 41900\tAverage Score: 108.86\n",
      "Episode 42000\tAverage Score: 143.14\n",
      "Episode 42100\tAverage Score: 192.85\n",
      "Episode 42200\tAverage Score: 217.64\n",
      "Episode 42300\tAverage Score: 226.93\n",
      "Episode 42400\tAverage Score: 227.43\n",
      "Episode 42500\tAverage Score: 221.90\n",
      "Episode 42600\tAverage Score: 207.24\n",
      "Episode 42700\tAverage Score: 215.00\n",
      "Episode 42800\tAverage Score: 341.91\n",
      "Episode 42900\tAverage Score: 497.81\n",
      "Episode 43000\tAverage Score: 464.01\n",
      "Episode 43100\tAverage Score: 497.61\n",
      "Episode 43200\tAverage Score: 335.66\n",
      "Episode 43300\tAverage Score: 287.47\n",
      "Episode 43400\tAverage Score: 389.23\n",
      "Episode 43500\tAverage Score: 500.00\n",
      "Episode 43600\tAverage Score: 500.00\n",
      "Episode 43700\tAverage Score: 500.00\n",
      "Episode 43800\tAverage Score: 447.88\n",
      "Episode 43900\tAverage Score: 290.98\n",
      "Episode 44000\tAverage Score: 500.00\n",
      "Episode 44100\tAverage Score: 500.00\n",
      "Episode 44200\tAverage Score: 500.00\n",
      "Episode 44300\tAverage Score: 500.00\n",
      "Episode 44400\tAverage Score: 500.00\n",
      "Episode 44500\tAverage Score: 500.00\n",
      "Episode 44600\tAverage Score: 500.00\n",
      "Episode 44700\tAverage Score: 500.00\n",
      "Episode 44800\tAverage Score: 500.00\n",
      "Episode 44900\tAverage Score: 500.00\n",
      "Episode 45000\tAverage Score: 500.00\n",
      "Episode 45100\tAverage Score: 500.00\n",
      "Episode 45200\tAverage Score: 500.00\n",
      "Episode 45300\tAverage Score: 500.00\n",
      "Episode 45400\tAverage Score: 500.00\n",
      "Episode 45500\tAverage Score: 500.00\n",
      "Episode 45600\tAverage Score: 500.00\n",
      "Episode 45700\tAverage Score: 500.00\n",
      "Episode 45800\tAverage Score: 500.00\n",
      "Episode 45900\tAverage Score: 500.00\n",
      "Episode 46000\tAverage Score: 500.00\n",
      "Episode 46100\tAverage Score: 500.00\n",
      "Episode 46200\tAverage Score: 500.00\n",
      "Episode 46300\tAverage Score: 500.00\n",
      "Episode 46400\tAverage Score: 500.00\n",
      "Episode 46500\tAverage Score: 500.00\n",
      "Episode 46600\tAverage Score: 500.00\n",
      "Episode 46700\tAverage Score: 500.00\n",
      "Episode 46800\tAverage Score: 500.00\n",
      "Episode 46900\tAverage Score: 500.00\n",
      "Episode 47000\tAverage Score: 500.00\n",
      "Episode 47100\tAverage Score: 500.00\n",
      "Episode 47200\tAverage Score: 500.00\n",
      "Episode 47300\tAverage Score: 500.00\n",
      "Episode 47400\tAverage Score: 500.00\n",
      "Episode 47500\tAverage Score: 500.00\n",
      "Episode 47600\tAverage Score: 500.00\n",
      "Episode 47700\tAverage Score: 500.00\n",
      "Episode 47800\tAverage Score: 500.00\n",
      "Episode 47900\tAverage Score: 500.00\n",
      "Episode 48000\tAverage Score: 500.00\n",
      "Episode 48100\tAverage Score: 500.00\n",
      "Episode 48200\tAverage Score: 500.00\n",
      "Episode 48300\tAverage Score: 500.00\n",
      "Episode 48400\tAverage Score: 500.00\n",
      "Episode 48500\tAverage Score: 500.00\n",
      "Episode 48600\tAverage Score: 500.00\n",
      "Episode 48700\tAverage Score: 500.00\n",
      "Episode 48800\tAverage Score: 500.00\n",
      "Episode 48900\tAverage Score: 500.00\n",
      "Episode 49000\tAverage Score: 500.00\n",
      "Episode 49100\tAverage Score: 500.00\n",
      "Episode 49200\tAverage Score: 500.00\n",
      "Episode 49300\tAverage Score: 500.00\n",
      "Episode 49400\tAverage Score: 500.00\n",
      "Episode 49500\tAverage Score: 500.00\n",
      "Episode 49600\tAverage Score: 500.00\n",
      "Episode 49700\tAverage Score: 500.00\n",
      "Episode 49800\tAverage Score: 500.00\n",
      "Episode 49900\tAverage Score: 500.00\n",
      "Episode 50000\tAverage Score: 500.00\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"],\n",
    "                   100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3FamHmxyhBEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ohGSXDyHh0xx",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(eval_env,\n",
    "               cartpole_hyperparameters[\"max_t\"],\n",
    "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               cartpole_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(cartpole_policy, \"./4-cart-pole.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LIVsvlW_8tcw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Lo4JH45if81z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a replay video of the agent\n",
    "    :param env\n",
    "    :param Qtable: Qtable of our agent\n",
    "    :param out_directory\n",
    "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _ = policy.act(state)\n",
    "        state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x56714c0] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    }
   ],
   "source": [
    "record_video(eval_env, cartpole_policy, \"./4-cart-pole.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls autoplay><source src=\"./4-cart-pole.mp4\" type=\"video/mp4\"></video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"./4-cart-pole.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLVmKKVKA6j"
   },
   "source": [
    "## Second agent: PixelCopter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JBSc8mlfyin3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "L5u_zAHsKBy7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  7\n",
      "Sample observation [-0.6999531  -0.99876493 -0.21001318  1.0711119  -2.3947988  -1.2707206\n",
      " -0.96637326]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "D7yJM9YXKNbq",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  2\n",
      "Action Space Sample 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wrNuVcHC8Xu7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "y0uujOR_ypB6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7mM2P_ckysFE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(50)\n",
    "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "v1HEqP-fy-Rf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage Score: 4.32\n",
      "Episode 2000\tAverage Score: 4.45\n",
      "Episode 3000\tAverage Score: 7.15\n",
      "Episode 4000\tAverage Score: 12.55\n",
      "Episode 5000\tAverage Score: 13.12\n",
      "Episode 6000\tAverage Score: 12.67\n",
      "Episode 7000\tAverage Score: 16.58\n",
      "Episode 8000\tAverage Score: 21.05\n",
      "Episode 9000\tAverage Score: 23.51\n",
      "Episode 10000\tAverage Score: 26.23\n",
      "Episode 11000\tAverage Score: 21.49\n",
      "Episode 12000\tAverage Score: 26.59\n",
      "Episode 13000\tAverage Score: 21.72\n",
      "Episode 14000\tAverage Score: 20.74\n",
      "Episode 15000\tAverage Score: 27.50\n",
      "Episode 16000\tAverage Score: 22.38\n",
      "Episode 17000\tAverage Score: 25.81\n",
      "Episode 18000\tAverage Score: 25.81\n",
      "Episode 19000\tAverage Score: 17.84\n",
      "Episode 20000\tAverage Score: 30.01\n",
      "Episode 21000\tAverage Score: 29.25\n",
      "Episode 22000\tAverage Score: 35.58\n",
      "Episode 23000\tAverage Score: 30.52\n",
      "Episode 24000\tAverage Score: 33.40\n",
      "Episode 25000\tAverage Score: 25.08\n",
      "Episode 26000\tAverage Score: 26.02\n",
      "Episode 27000\tAverage Score: 26.13\n",
      "Episode 28000\tAverage Score: 40.24\n",
      "Episode 29000\tAverage Score: 17.60\n",
      "Episode 30000\tAverage Score: 24.33\n",
      "Episode 31000\tAverage Score: 23.56\n",
      "Episode 32000\tAverage Score: 33.92\n",
      "Episode 33000\tAverage Score: 35.97\n",
      "Episode 34000\tAverage Score: 28.07\n",
      "Episode 35000\tAverage Score: 27.18\n",
      "Episode 36000\tAverage Score: 29.76\n",
      "Episode 37000\tAverage Score: 36.72\n",
      "Episode 38000\tAverage Score: 30.98\n",
      "Episode 39000\tAverage Score: 36.41\n",
      "Episode 40000\tAverage Score: 35.03\n",
      "Episode 41000\tAverage Score: 40.24\n",
      "Episode 42000\tAverage Score: 36.11\n",
      "Episode 43000\tAverage Score: 37.73\n",
      "Episode 44000\tAverage Score: 35.99\n",
      "Episode 45000\tAverage Score: 46.54\n",
      "Episode 46000\tAverage Score: 47.72\n",
      "Episode 47000\tAverage Score: 38.81\n",
      "Episode 48000\tAverage Score: 44.37\n",
      "Episode 49000\tAverage Score: 34.32\n",
      "Episode 50000\tAverage Score: 51.43\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(pixelcopter_policy,\n",
    "                   pixelcopter_optimizer,\n",
    "                   pixelcopter_hyperparameters[\"n_training_episodes\"],\n",
    "                   pixelcopter_hyperparameters[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters[\"gamma\"],\n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34.1, 22.77037549097511)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(eval_env,\n",
    "               pixelcopter_hyperparameters[\"max_t\"],\n",
    "               pixelcopter_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               pixelcopter_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(pixelcopter_policy, \"./4-pixel-copter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "record_video(eval_env, pixelcopter_policy, \"./4-pixel-copter.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls autoplay><source src=\"./4-pixel-copter.mp4\" type=\"video/mp4\"></video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"./4-pixel-copter.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BPLwsPajb1f8",
    "L_WSo0VUV99t",
    "mjY-eq3eWh9O",
    "JoTC9o2SczNn",
    "gfGJNZBUP7Vn",
    "YB0Cxrw1StrP",
    "47iuAFqV8Ws-",
    "x62pP0PHdA-y"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
